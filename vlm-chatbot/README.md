# VLM Chatbot

Qwen2.5-VL ê¸°ë°˜ ë©€í‹°ëª¨ë‹¬ ì±—ë´‡ - í”„ë¡ íŠ¸ì—”ë“œ/ë°±ì—”ë“œ ë¶„ë¦¬ êµ¬ì¡°

## í”„ë¡œì íŠ¸ ê°œìš”

ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ë™ì‹œì— ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë¹„ì „-ì–¸ì–´ ëª¨ë¸(VLM) ê¸°ë°˜ ì±—ë´‡ì…ë‹ˆë‹¤.
ìœ ì§€ë³´ìˆ˜ê°€ ìš©ì´í•œ í”„ë¡ íŠ¸ì—”ë“œ/ë°±ì—”ë“œ ë¶„ë¦¬ êµ¬ì¡°ë¡œ ì„¤ê³„ë˜ì–´, ì¶”í›„ ê²Œì‹œíŒ ë“±ì˜ ê¸°ëŠ¥ì„ ì‰½ê²Œ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ê¸°ìˆ  ìŠ¤íƒ

### ë°±ì—”ë“œ
- **FastAPI**: ê³ ì„±ëŠ¥ ë¹„ë™ê¸° ì›¹ í”„ë ˆì„ì›Œí¬
- **OpenAI Client**: vLLM ì„œë²„ ì—°ë™
- **Pydantic**: ë°ì´í„° ê²€ì¦ ë° ì„¤ì • ê´€ë¦¬
- **Uvicorn**: ASGI ì„œë²„

### í”„ë¡ íŠ¸ì—”ë“œ
- **React 18**: UI ë¼ì´ë¸ŒëŸ¬ë¦¬
- **Vite**: ë¹Œë“œ ë„êµ¬
- **Axios**: HTTP í´ë¼ì´ì–¸íŠ¸

### AI ëª¨ë¸
- **Qwen2.5-VL-7B**: Alibabaì˜ ë©€í‹°ëª¨ë‹¬ ì–¸ì–´ ëª¨ë¸
- **vLLM**: ê³ ì„±ëŠ¥ LLM ì¶”ë¡  ì—”ì§„

## í”„ë¡œì íŠ¸ êµ¬ì¡°

```
vlm-chatbot/
â”œâ”€â”€ backend/                 # FastAPI ë°±ì—”ë“œ
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py         # FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜
â”‚   â”‚   â”œâ”€â”€ config.py       # ì„¤ì • ê´€ë¦¬
â”‚   â”‚   â”œâ”€â”€ api/            # API ì—”ë“œí¬ì¸íŠ¸
â”‚   â”‚   â”œâ”€â”€ models/         # ë°ì´í„° ëª¨ë¸
â”‚   â”‚   â”œâ”€â”€ services/       # ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
â”‚   â”‚   â””â”€â”€ core/           # í•µì‹¬ ìœ í‹¸ë¦¬í‹°
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ frontend/               # React í”„ë¡ íŠ¸ì—”ë“œ
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/    # React ì»´í¬ë„ŒíŠ¸
â”‚   â”‚   â”œâ”€â”€ api/           # API í´ë¼ì´ì–¸íŠ¸
â”‚   â”‚   â”œâ”€â”€ styles/        # CSS ìŠ¤íƒ€ì¼
â”‚   â”‚   â””â”€â”€ App.jsx
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ë¹ ë¥¸ ì‹œì‘

### ì‚¬ì „ ìš”êµ¬ì‚¬í•­

1. **Anaconda** ì„¤ì¹˜
2. **Conda í™˜ê²½ `safe_qwen`** í™œì„±í™”
3. **Node.js 16+** ì„¤ì¹˜
4. **GPU í™˜ê²½** (CUDA ì§€ì›)

### 1. vLLM ì„œë²„ ì‹¤í–‰

```bash
# ì•„ë‚˜ì½˜ë‹¤ í™˜ê²½ í™œì„±í™”
conda activate safe_qwen

# vLLM ì„¤ì¹˜ (ì•„ì§ ì„¤ì¹˜í•˜ì§€ ì•Šì€ ê²½ìš°)
pip install vllm

# Qwen2.5-VL ëª¨ë¸ë¡œ vLLM ì„œë²„ ì‹œì‘ (ê¸°ë³¸)
vllm serve Qwen/Qwen2.5-VL-7B-Instruct --port 8000

# GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•œ ê²½ìš°: Tensor ë³‘ë ¬í™” ì‚¬ìš© (2ê°œ GPU)
vllm serve Qwen/Qwen2.5-VL-7B-Instruct --port 8000 --tensor-parallel-size 2

# GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•œ ê²½ìš°: 4ê°œ GPUë¡œ ë¶„ì‚°
vllm serve Qwen/Qwen2.5-VL-7B-Instruct --port 8000 --tensor-parallel-size 4

# ì¶”ê°€ ë©”ëª¨ë¦¬ ìµœì í™” ì˜µì…˜ (ê¶Œì¥)
vllm serve Qwen/Qwen2.5-VL-7B-Instruct \
  --port 8000 \
  --tensor-parallel-size 2 \
  --gpu-memory-utilization 0.9 \
  --max-model-len 4096
```

**vLLM ì„œë²„ ì˜µì…˜ ì„¤ëª…:**
- `--tensor-parallel-size N`: ëª¨ë¸ì„ Nê°œì˜ GPUì— ë¶„ì‚° (ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ í•„ìˆ˜)
- `--gpu-memory-utilization 0.9`: GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  (ê¸°ë³¸ 0.9 = 90%)
- `--max-model-len`: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ ì œí•œ (ë©”ëª¨ë¦¬ ì ˆì•½)
- `--dtype auto`: ìë™ ë°ì´í„° íƒ€ì… ì„ íƒ (fp16, bfloat16 ë“±)

### 2. ë°±ì—”ë“œ ì‹¤í–‰

```bash
# ì•„ë‚˜ì½˜ë‹¤ í™˜ê²½ í™œì„±í™”
conda activate safe_qwen

# ë°±ì—”ë“œ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd vlm-chatbot/backend

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ì„ íƒì‚¬í•­)
cp .env.example .env

# ì„œë²„ ì‹¤í–‰
python -m app.main
```

ë°±ì—”ë“œ ì„œë²„ê°€ http://127.0.0.1:8080ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.

### 3. í”„ë¡ íŠ¸ì—”ë“œ ì‹¤í–‰

ìƒˆ í„°ë¯¸ë„ì—ì„œ:

```bash
# í”„ë¡ íŠ¸ì—”ë“œ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd vlm-chatbot/frontend

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
npm install

# ê°œë°œ ì„œë²„ ì‹¤í–‰
npm run dev
```

í”„ë¡ íŠ¸ì—”ë“œê°€ http://localhost:3000ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.

## ì‚¬ìš© ë°©ë²•

1. ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:3000 ì ‘ì†
2. "ğŸ“· ì´ë¯¸ì§€ ì—…ë¡œë“œ" ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ì´ë¯¸ì§€ ì„ íƒ
3. ë©”ì‹œì§€ ì…ë ¥ë€ì— ì§ˆë¬¸ ì…ë ¥ (ì˜ˆ: "ì´ ì´ë¯¸ì§€ì—ì„œ ìœ„í—˜ìš”ì†Œë¥¼ ì°¾ì•„ì¤˜")
4. "ë³´ë‚´ê¸°" ë²„íŠ¼ í´ë¦­ ë˜ëŠ” Enter í‚¤ ì…ë ¥
5. AIì˜ ì‘ë‹µ í™•ì¸

## API ë¬¸ì„œ

ë°±ì—”ë“œ ì„œë²„ ì‹¤í–‰ í›„ ë‹¤ìŒ URLì—ì„œ API ë¬¸ì„œ í™•ì¸:

- **Swagger UI**: http://127.0.0.1:8080/docs
- **ReDoc**: http://127.0.0.1:8080/redoc

## ì£¼ìš” ê¸°ëŠ¥

### í˜„ì¬ êµ¬í˜„ëœ ê¸°ëŠ¥

- âœ… ë©€í‹°ëª¨ë‹¬ ì±„íŒ… (ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸)
- âœ… ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬
- âœ… ì´ë¯¸ì§€ ì—…ë¡œë“œ ë° ë¯¸ë¦¬ë³´ê¸°
- âœ… ë°˜ì‘í˜• UI
- âœ… ì‹¤ì‹œê°„ ë©”ì‹œì§€ ì „ì†¡
- âœ… ë¡œë”© ìƒíƒœ í‘œì‹œ

### í™•ì¥ ê°€ëŠ¥í•œ ê¸°ëŠ¥ (ì¶”í›„ êµ¬í˜„)

- ğŸ”² ì‚¬ìš©ì ì¸ì¦ ë° ê¶Œí•œ ê´€ë¦¬
- ğŸ”² ê²Œì‹œíŒ ê¸°ëŠ¥
- ğŸ”² ì±„íŒ… íˆìŠ¤í† ë¦¬ ì €ì¥ (ë°ì´í„°ë² ì´ìŠ¤)
- ğŸ”² íŒŒì¼ ê´€ë¦¬ ì‹œìŠ¤í…œ
- ğŸ”² ê´€ë¦¬ì ëŒ€ì‹œë³´ë“œ
- ğŸ”² ë‹¤ì¤‘ ì‚¬ìš©ì ì§€ì›
- ğŸ”² WebSocket ê¸°ë°˜ ì‹¤ì‹œê°„ ì±„íŒ…

## ê°œë°œ ê°€ì´ë“œ

### ë°±ì—”ë“œ í™•ì¥

ê²Œì‹œíŒ API ì¶”ê°€ ì˜ˆì‹œ:

```bash
cd backend/app/api
# board.py ìƒì„±
```

```python
# app/api/board.py
from fastapi import APIRouter

router = APIRouter(prefix="/api/board", tags=["board"])

@router.get("/posts")
async def get_posts():
    return {"posts": []}
```

`app/main.py`ì— ë¼ìš°í„° ì¶”ê°€:

```python
from app.api import chat, board

app.include_router(chat.router)
app.include_router(board.router)
```

### í”„ë¡ íŠ¸ì—”ë“œ í™•ì¥

ìƒˆ í˜ì´ì§€ ì¶”ê°€ë¥¼ ìœ„í•œ ë¼ìš°íŒ…:

```bash
cd frontend
npm install react-router-dom
```

```jsx
// src/App.jsx
import { BrowserRouter, Routes, Route } from 'react-router-dom'
import ChatInterface from './components/ChatInterface'
import Board from './components/Board'

function App() {
  return (
    <BrowserRouter>
      <Routes>
        <Route path="/" element={<ChatInterface />} />
        <Route path="/board" element={<Board />} />
      </Routes>
    </BrowserRouter>
  )
}
```

## ì„¤ì •

### ë°±ì—”ë“œ ì„¤ì •

`backend/.env` íŒŒì¼ì—ì„œ ì„¤ì •:

```env
VLM_BASE_URL=http://127.0.0.1:8000/v1
VLM_MODEL=qwen2.5-vl-7b
API_PORT=8080
```

### í”„ë¡ íŠ¸ì—”ë“œ ì„¤ì •

`frontend/vite.config.js`ì—ì„œ í”„ë¡ì‹œ ì„¤ì •:

```javascript
export default defineConfig({
  server: {
    proxy: {
      '/api': 'http://127.0.0.1:8080'
    }
  }
})
```

## ë°°í¬

### í”„ë¡œë•ì…˜ ë¹Œë“œ

```bash
# í”„ë¡ íŠ¸ì—”ë“œ ë¹Œë“œ
cd frontend
npm run build

# ë¹Œë“œëœ íŒŒì¼ì€ frontend/dist/ì— ìƒì„±ë¨
```

### Docker ë°°í¬ (ì„ íƒì‚¬í•­)

ì¶”í›„ Docker Composeë¥¼ ì‚¬ìš©í•œ ë°°í¬ ì§€ì› ì˜ˆì •

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### vLLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨

```
Error: Connection refused
```

â†’ vLLM ì„œë²„ê°€ http://127.0.0.1:8000ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸

### ë°±ì—”ë“œ ì‹¤í–‰ ì˜¤ë¥˜

```
ModuleNotFoundError: No module named 'fastapi'
```

â†’ ì•„ë‚˜ì½˜ë‹¤ í™˜ê²½ í™œì„±í™” ë° íŒ¨í‚¤ì§€ ì¬ì„¤ì¹˜:
```bash
conda activate safe_qwen
pip install -r requirements.txt
```

### í”„ë¡ íŠ¸ì—”ë“œ ë¹Œë“œ ì˜¤ë¥˜

```
npm ERR! code ELIFECYCLE
```

â†’ node_modules ì¬ì„¤ì¹˜:
```bash
rm -rf node_modules package-lock.json
npm install
```

### CORS ì˜¤ë¥˜

```
Access to XMLHttpRequest has been blocked by CORS policy
```

â†’ ë°±ì—”ë“œ `.env`ì˜ `CORS_ORIGINS`ì— í”„ë¡ íŠ¸ì—”ë“œ URL ì¶”ê°€

### GPU ë©”ëª¨ë¦¬ ë¶€ì¡± (OOM)

```
torch.cuda.OutOfMemoryError: CUDA out of memory
```

**í•´ê²° ë°©ë²•:**

1. **Tensor ë³‘ë ¬í™” ì‚¬ìš©** (ê°€ì¥ íš¨ê³¼ì )
   ```bash
   # GPU 2ê°œ ì‚¬ìš©
   vllm serve Qwen/Qwen2.5-VL-7B-Instruct --port 8000 --tensor-parallel-size 2
   ```

2. **GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì¡°ì •**
   ```bash
   vllm serve Qwen/Qwen2.5-VL-7B-Instruct --port 8000 --gpu-memory-utilization 0.8
   ```

3. **ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ ì œí•œ**
   ```bash
   vllm serve Qwen/Qwen2.5-VL-7B-Instruct --port 8000 --max-model-len 2048
   ```

4. **ì‚¬ìš© ê°€ëŠ¥í•œ GPU í™•ì¸**
   ```bash
   nvidia-smi
   # ë˜ëŠ”
   python -c "import torch; print(f'GPU ê°œìˆ˜: {torch.cuda.device_count()}')"
   ```

5. **ê¶Œì¥ ì„¤ì • (GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ)**
   ```bash
   vllm serve Qwen/Qwen2.5-VL-7B-Instruct \
     --port 8000 \
     --tensor-parallel-size 2 \
     --gpu-memory-utilization 0.85 \
     --max-model-len 4096 \
     --dtype half
   ```

## ë¼ì´ì„ ìŠ¤

MIT

## ê¸°ì—¬

ì´ìŠˆ ë° í’€ ë¦¬í€˜ìŠ¤íŠ¸ë¥¼ í™˜ì˜í•©ë‹ˆë‹¤!

## ì°¸ê³ 

- [FastAPI ë¬¸ì„œ](https://fastapi.tiangolo.com/)
- [React ë¬¸ì„œ](https://react.dev/)
- [Qwen2.5-VL ëª¨ë¸](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct)
- [vLLM ë¬¸ì„œ](https://docs.vllm.ai/)
