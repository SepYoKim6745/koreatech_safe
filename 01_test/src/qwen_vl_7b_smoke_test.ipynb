{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c84fb9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9d3acd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c61d4737fd84486d9ad30c42077c42f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    }
   ],
   "source": [
    "# default: Load the model on the available device(s)\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-7B-Instruct\", dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# default processer\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feda8f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"../data/test1.jpeg\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"이 사진 속에서 위험요소를 찾아주고 개선방안도 알려줘. + \"}, \n",
    "        ],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e3a455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5763404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 사진에서 주요 위험 요소와 그에 따른 개선 방안은 다음과 같습니다:\n",
      "\n",
      "1. **전기 케이블**: 흰색 전기 케이블이 놓여 있으며, 이는 안전한 위치에 있지 않습니다. 특히, 케이블이 다른 물건 위에 놓여 있거나, 다른 물건과 접촉하는 경우, 화재의 위험이 있습니다.\n",
      "\n",
      "   - **개선 방안**: 케이블을 안전하게 관리하고, 다른 물건과 분리하여 놓아야 합니다. 필요하다면, 케이블을 바닥에 고정하거나, 케이블 관을 사용하여 관리할 수 있습니다.\n",
      "\n",
      "2. **전자제품의 위치**: 전자제품들이 모두 같은 공간에 놓여 있습니다. 이는 화재나 전기 누설의 위험을 증가시킬 수 있습니다.\n",
      "\n",
      "   - **개선 방안**: 각 전자제품이 따로 있는 공간에 놓여야 합니다. 예를 들어, 미니 오븐은 다른 공간에, 커피 머신은 다른 공간에 놓여야 합니다. 또한, 각 장치가 충분한 공간을 가지고 있어야 하며, 충분한 공기가 통과할 수 있도록 해야 합니다.\n",
      "\n",
      "3. **물건의 위치**: 물건들이 미끄러지거나 넘어질 위험이 있습니다. 특히, 커피 머신과 커피 컵이 같은 공간에 놓여 있습니다.\n",
      "\n",
      "   - **개선 방안**: 물건들이 미끄러지거나 넘어질 위험을 줄이기 위해, 물건들이 안정적으로 놓여야 합니다. 필요하다면, 물건들을 안전하게 고정하거나, 물건들이 미끄러지지 않도록 하는 방법을 찾아야 합니다.\n",
      "\n",
      "4. **화재 위험**: 미니 오븐과 커피 머신이 같은 공간에 놓여 있으며, 이는 화재의 위험을 증가시킬 수 있습니다.\n",
      "\n",
      "   - **개선 방안**: 각 전자제품이 따로 있는 공간에 놓여야 합니다. 또한, 전자제품들이 충분한 공간을 가지고 있어야 하며, 충분한 공기가 통과할 수 있도록 해야 합니다. 또한, 전자제품들이 서로 가까워지지 않도록 하여 화재의 위험을 줄일 수 있습니다.\n",
      "\n",
      "이러한 위험 요소를 해결하기 위해서는 전문가의 도움을 받는 것이 좋습니다.\n"
     ]
    }
   ],
   "source": [
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "# print(output_text)\n",
    "\n",
    "for text in output_text:\n",
    "    print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safe_qwen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
